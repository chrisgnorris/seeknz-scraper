{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seek NZ Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create a file with Jupyter and try and scrape Seek.co.nz\n",
    "###### Used https://www.youtube.com/watch?v=eN_3d4JrL_w as a base\n",
    "###### https://github.com/chrisgnorris/seeknz-scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_url(position, location):\n",
    "    \"\"\"Generate a url from position and location\"\"\"\n",
    "    template = 'https://www.seek.co.nz/{}-jobs/in-All-{}'\n",
    "    url = template.format(position, location)\n",
    "    return url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = get_url('data-scientist','New-Zealand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract raw html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response.reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cards = soup.find_all('article')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "section = soup.find('div', {'class':'_3MPUOLE'})\n",
    "x = 0\n",
    "for div in section.select('div[data-search-sol-meta]'):\n",
    "    x = x + 1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs = soup.find_all('div[data-search-sol-meta]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Try to get Proxy via VPN working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Currently not working - obviously depends on the VPN provider and can be sticked. You can always just turn the VPN on\n",
    "#Or you use the proxy - even some free ones https://pypi.org/project/free-proxy/\n",
    "#This isn't required!!!\n",
    "\n",
    "# https://dev.to/thughes24/how-to-turn-your-vpn-into-a-proxy-using-python-28ag\n",
    "proxy = {\n",
    "    'http': \"test@gmail.com:password123@nz-akl.prod.surfshark.com\",\n",
    "    'https': \"test@gmail.com:password123@nz-akl.prod.surfshark.com\"\n",
    "}\n",
    "response2 = requests.get('https://google.com',proxies=proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response2.reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use shadow socks from Surfshark\n",
    "# https://stackoverflow.com/questions/56934030/how-do-i-use-nordvpn-servers-as-python-requests-proxies/65441742#65441742"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype the model with a single record\n",
    "#### I.e the way from the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "card = cards[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "atag = card.h1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_title = atag.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_url = 'https://www.seek.co.nz' + atag.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "company = card.find('span',{\"_3FrNV7v _3PZrylH E6m4BZb\"}).a.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "card.find('span',{\"_3FrNV7v _3PZrylH E6m4BZb\"}).text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "location = card.find('div',{'class':'xxz8a1h'}).a.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salary = card.find('span',{'class':'lwHBT6d'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "card.find_all('span',{'class':{'Eadjc1o' : 'location'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobcategory = card.find(attrs={\"data-automation\": \"jobClassification\"}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype the model with a single record V2\n",
    "#### I.e the improved way I created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "card = cards[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_title = card.find(attrs={\"data-automation\": \"jobTitle\"}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_url = 'https://www.seek.co.nz' + card.find(attrs={\"data-automation\": \"jobTitle\"}).get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    company = card.find(attrs={\"data-automation\": \"jobCompany\"}).text\n",
    "except AttributeError:\n",
    "    company = ''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "location = card.find(attrs={\"data-automation\": \"jobLocation\"}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    job_salary = card.find(attrs={\"data-automation\": \"jobSalary\"}).text\n",
    "except AttributeError:\n",
    "    job_salary = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_category = card.find(attrs={\"data-automation\": \"jobClassification\"}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_subcategory = card.find(attrs={\"data-automation\": \"jobSubClassification\"}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_short_description = card.find(attrs={\"data-automation\": \"jobShortDescription\"}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    job_listing_date = card.find(attrs={\"data-automation\": \"jobListingDate\"}).text\n",
    "except AttributeError:\n",
    "    job_listing_date = 'Featured'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_mined = datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_id = card.get('data-job-id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bullet_points = ''\n",
    "x = 0\n",
    "for li in card.select('li'):\n",
    "    if x == 0:\n",
    "        bullet_points = bullet_points + li.text\n",
    "        x = 1\n",
    "    else:\n",
    "        bullet_points = bullet_points + ' - ' + li.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalise the model with a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_record(card):\n",
    "    \"\"\"Extract job data from a single record\"\"\"\n",
    "    card = cards[0]\n",
    "    job_title = card.find(attrs={\"data-automation\": \"jobTitle\"}).text\n",
    "    job_url = 'https://www.seek.co.nz' + card.find(attrs={\"data-automation\": \"jobTitle\"}).get('href')\n",
    "    company = card.find(attrs={\"data-automation\": \"jobCompany\"}).text\n",
    "    location = card.find(attrs={\"data-automation\": \"jobLocation\"}).text\n",
    "    try:\n",
    "        job_salary = card.find(attrs={\"data-automation\": \"jobSalary\"}).text\n",
    "    except AttributeError:\n",
    "        job_salary = ''\n",
    "    job_category = card.find(attrs={\"data-automation\": \"jobClassification\"}).text\n",
    "    job_subcategory = card.find(attrs={\"data-automation\": \"jobSubClassification\"}).text\n",
    "    job_short_description = card.find(attrs={\"data-automation\": \"jobShortDescription\"}).text\n",
    "    try:\n",
    "        job_listing_date = card.find(attrs={\"data-automation\": \"jobListingDate\"}).text\n",
    "    except AttributeError:\n",
    "        job_listing_date = 'Featured'\n",
    "    date_mined = datetime.today().strftime('%Y-%m-%d')\n",
    "    job_id = card.get('data-job-id')\n",
    "    bullet_points = ''\n",
    "    x = 0\n",
    "    for li in card.select('li'):\n",
    "        if x == 0:\n",
    "            bullet_points = bullet_points + li.text\n",
    "            x = 1\n",
    "        else:\n",
    "            bullet_points = bullet_points + ' - ' + li.text\n",
    "            \n",
    "    record = (job_id,job_title,company,location,jobshortdescription,bullet_points,job_salary,jobcategory,jobsubcategory,job_url)\n",
    "    \n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "for card in cards:\n",
    "    record = get_record(card)\n",
    "    records.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(records[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the next page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        url = 'https://www.seek.co.nz' + soup.find(attrs={\"data-automation\": \"page-next\"}).get('href')\n",
    "    except AttributeError:\n",
    "        break\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    cards = soup.find_all('article')\n",
    "    \n",
    "    for card in cards:\n",
    "    record = get_record(card)\n",
    "    records.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(records))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving File to Excel and referencing as a dataframe to ignore previously mined files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Opening existing file as DB\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('results.csv')  \n",
    "except FileNotFoundError:\n",
    "    print('No existing file found')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/60675117/returning-a-string-from-loc-query\n",
    "#https://stackoverflow.com/questions/56260348/selecting-single-value-in-a-pandas-dataframe\n",
    "lookup = df.loc[df['ID'] == '1111', 'ID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if lookup.size > 0:\n",
    "    print('Found')\n",
    "else:\n",
    "    print('Not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Put together\n",
    "def lookup_ID(jobid):\n",
    "    \"\"\"Feed URL for it to look up, will return found or not found\"\"\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv('results.csv')  \n",
    "        lookup = df.loc[df['ID'] == jobid, 'ID'].values\n",
    "        \n",
    "        if lookup.size > 0:\n",
    "            searchResult = 'Found'\n",
    "         \n",
    "        else:\n",
    "            searchResult = 'Not found'\n",
    "        \n",
    "        return searchResult\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        searchResult = 'No existing csv file found'\n",
    "        return searchResult\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lookup_ID(51160410)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mine hidden salary data from Seek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/37465172/how-to-use-beautifulsoup-to-scrape-a-webpage-url\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_url_salary(jobid_salary):\n",
    "    \"\"\"Generate a url to get salary info from seek job id\"\"\"\n",
    "    template = 'https://qdjrmx4vb1.execute-api.ap-southeast-2.amazonaws.com/chickennuggets?jobId={}'\n",
    "    url = template.format(jobid_salary)\n",
    "    return url\n",
    "\n",
    "get_url_salary(51155454)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salurl = get_url_salary(51155454) # get url\n",
    "\n",
    "response = requests.get(salurl) # get request\n",
    "r = response.json() # convert the json array text to dictionary\n",
    "\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(r[\"jobInfoArray\"])\n",
    "print(r['jobInfoArray'][0]) #https://stackoverflow.com/questions/51788550/parsing-json-nested-dictionary-using-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r[\"jobInfoArray\"][0]['upperLimit'] # dict name,item #, item name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salaryrange = str(r[\"jobInfoArray\"][0]['upperLimit']) # dict name,item #, item name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salaryrange = salaryrange.replace(',', '') # Removes comma so can turn into int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salaryrange = salaryrange.replace('-', ' ').split(' ') # splits text based on dash or space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salaryrangelow = int(salaryrange[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salaryrangehigh = int(salaryrange[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(salaryrangelow)\n",
    "print(salaryrangehigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combining it into a function\n",
    "\n",
    "def get_salary(jobid_salary):\n",
    "    \"\"\"Generate a url to get salary info from seek job id and returns data as tuple with low and high\"\"\"\n",
    "    \n",
    "    template = 'https://qdjrmx4vb1.execute-api.ap-southeast-2.amazonaws.com/chickennuggets?jobId={}'\n",
    "    salurl = template.format(jobid_salary)\n",
    "    \n",
    "    \n",
    "    response = requests.get(salurl) # get request\n",
    "    r = response.json() # convert the json array text to dictionary\n",
    "    try:\n",
    "        r[\"jobInfoArray\"][0]['upperLimit'] # dict name,item #, item name \n",
    "        salaryrange = str(r[\"jobInfoArray\"][0]['upperLimit']) # dict name,item #, item name\n",
    "        salaryrange = salaryrange.replace(',', '') # Removes comma so can turn into int\n",
    "        salaryrange = salaryrange.replace('-', ' ').split(' ') # splits text based on dash or space\n",
    "        salaryrangelow = int(salaryrange[0])\n",
    "        salaryrangehigh = int(salaryrange[1])\n",
    "    \n",
    "        return salaryrangelow,salaryrangehigh # return as a tuple - get away from global variables!\n",
    "    \n",
    "    except KeyError: #If it can't find job, will return 0\n",
    "        salaryrangelow = int(0)\n",
    "        salaryrangehigh = int(0)\n",
    "    \n",
    "        return salaryrangelow,salaryrangehigh # return as a tuple - get away from global variables!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salaryresult = get_salary(51155454)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(salaryresult[0])\n",
    "print(salaryresult[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mine Keywords and Bullet Points within Listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Goal is to figure out key skills needed - what experience etc. \n",
    "# Should be able to just mine bullet points and add as string. \n",
    "\n",
    "# Will briefly look at some of info I emailed to myself originally about big data text mining\n",
    "# I could potentially grab ALL text. Then strip out a manual list of useless words, then left with the keywords. Similar to how a summary bot would work. \n",
    "\n",
    "# This could lead to some fun word clouds\n",
    "\n",
    "#1. Get the text\n",
    "#2. Have fun analysing with the bottom information - all have links to code and talk about keyword analysing libraries\n",
    "#3. Probably start by just having a quick Google about analysing text for keywords etc\n",
    "\n",
    "# https://towardsdatascience.com/python-vs-r-what-i-learned-from-4-000-job-advertisements-ab41661b7f28\n",
    "# https://towardsdatascience.com/how-to-identify-the-most-requested-skills-on-the-data-science-job-market-with-data-science-726845ca9638\n",
    "# https://github.com/ppeyliang/web-scraping-job-postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://datascience.stackexchange.com/questions/50824/from-most-frequent-words-how-to-extract-technical-skill-words\n",
    "#https://medium.com/towards-artificial-intelligence/text-mining-in-python-steps-and-examples-78b3f8fd913b\n",
    "#https://nikkisharma536.medium.com/white-paper-job-skills-extraction-with-lstm-and-word-embeddings-d71d1f96024f\n",
    "#https://towardsdatascience.com/getting-started-with-text-analysis-in-python-ca13590eb4f7\n",
    "#https://tauvicr.wordpress.com/2018/10/07/skillmatch-extracting-facts-from-text/\n",
    "#https://smmry.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = requests.get('https://www.seek.co.nz/job/51210858?type=standout#searchRequestToken=77122757-c412-41cf-a46d-825a1acdf5c8')\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_id = card.get('data-job-id')\n",
    "page_text = soup.find(attrs={\"data-automation\": \"mobileTemplate\"}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might need to do this!!! As it cleans the text - similar to print but in the object\n",
    "\n",
    "# import unicodedata\n",
    "# page_text = unicodedata.normalize(\"NFKD\", job_subcategory)\n",
    "# page_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page_text = page_text.replace(r\"\\'\", r\"'\")\n",
    "print(page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints new lines formating\n",
    "# page_text = page_text.replace('\\xa0', '\\n')\n",
    "# print(page_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse Text - NLTK Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://datascience.stackexchange.com/questions/50824/from-most-frequent-words-how-to-extract-technical-skill-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse Text - Spacy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using spacy, should be able to train it for look to a list of keywords! You could reverse engineer the CV screeners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://tauvicr.wordpress.com/2018/10/07/skillmatch-extracting-facts-from-text/ \n",
    "# https://spacy.io/usage/spacy-101\n",
    "# https://realpython.com/natural-language-processing-spacy-python/\n",
    "# https://www.analyticsvidhya.com/blog/2019/09/introduction-information-extraction-python-spacy/\n",
    "# https://towardsdatascience.com/do-the-keywords-in-your-resume-aptly-represent-what-type-of-data-scientist-you-are-59134105ba0d\n",
    "# https://medium.com/better-programming/extract-keywords-using-spacy-in-python-4a8415478fbf\n",
    "# https://www.analyticsvidhya.com/blog/2020/03/spacy-tutorial-learn-natural-language-processing/\n",
    "\n",
    "import en_core_web_sm\n",
    "from collections import Counter\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints words and tokens - i.e this can help you figure out what needs to be trained\n",
    "doc = nlp(page_text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints organisations and products\n",
    "doc = nlp(page_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints sentences\n",
    "about_doc = nlp(page_text)\n",
    "sentences = list(about_doc.sents)\n",
    "len(sentences)\n",
    "for sentence in sentences:\n",
    "    print (sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reports common words and removes stop words\n",
    "\n",
    "complete_doc = nlp(page_text)\n",
    "# Remove stop words and punctuation symbols\n",
    "words = [token.text for token in complete_doc\n",
    "         if not token.is_stop and not token.is_punct]\n",
    "word_freq = Counter(words)\n",
    "# 5 commonly occurring words with their frequencies\n",
    "common_words = word_freq.most_common(5)\n",
    "print (common_words)\n",
    "\n",
    "# Unique words\n",
    "unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
    "print (unique_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return specific category of words\n",
    "\n",
    "# for token in about_doc:\n",
    "#     print (token, token.tag_, token.pos_, spacy.explain(token.tag_))\n",
    "\n",
    "nouns = []\n",
    "adjectives = []\n",
    "for token in about_doc:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns.append(token)\n",
    "    if token.pos_ == 'ADJ':\n",
    "        adjectives.append(token)\n",
    "\n",
    "nouns\n",
    "\n",
    "adjectives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words, punct, lowercases etc\n",
    "def is_token_allowed(token):\n",
    "    '''\n",
    "        Only allow valid tokens which are not stop words\n",
    "        and punctuation symbols.\n",
    "    '''\n",
    "    if (not token or not token.string.strip() or\n",
    "        token.is_stop or token.is_punct):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def preprocess_token(token):\n",
    "    # Reduce token to its lowercase lemma form\n",
    "    return token.lemma_.strip().lower()\n",
    "\n",
    "complete_filtered_tokens = [preprocess_token(token)\n",
    "    for token in complete_doc if is_token_allowed(token)]\n",
    "complete_filtered_tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conference_doc = nlp(page_text)\n",
    "# Extract Noun Phrases\n",
    "for chunk in conference_doc.noun_chunks:\n",
    "    print (chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse text by training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://confusedcoders.com/wp-content/uploads/2019/09/Job-Skills-extraction-with-LSTM-and-Word-Embeddings-Nikita-Sharma.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/getting-started-with-text-analysis-in-python-ca13590eb4f7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together - Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "#Global variables are telling Python that you are using these variables in the global pool. \n",
    "#Using them in local command can cause issues. Think of it as sending variables to the cloud, or local storage. I.e \n",
    "#sharepoint or saving on desktop\n",
    "#https://www.w3schools.com/python/python_variables_global.asp \n",
    "\n",
    "def lookup_id(jobid):\n",
    "    \"\"\"Feed ID for it to look up, will return found or not found\"\"\"\n",
    "    global df  #This is loaded in Main function, as not to load df everytime\n",
    "    lookup = df.loc[df['ID'] == jobid, 'ID'].values\n",
    "    \n",
    "    if lookup.size > 0:\n",
    "        searchResult = 'Found'\n",
    "    else:\n",
    "        searchResult = 'Not found'\n",
    "    \n",
    "    return searchResult\n",
    "\n",
    "def get_salary(jobid_salary): # https://www.whatsthesalary.com/\n",
    "    \"\"\"Generate a url to get salary info from seek job id and returns data as tuple with low and high\"\"\"\n",
    "    \n",
    "    template = 'https://qdjrmx4vb1.execute-api.ap-southeast-2.amazonaws.com/chickennuggets?jobId={}'\n",
    "    salurl = template.format(jobid_salary)\n",
    "    \n",
    "    \n",
    "    response = requests.get(salurl) # get request\n",
    "    r = response.json() # convert the json array text to dictionary\n",
    "    try:\n",
    "        r[\"jobInfoArray\"][0]['upperLimit'] # dict name,item #, item name \n",
    "        salaryrange = str(r[\"jobInfoArray\"][0]['upperLimit']) # dict name,item #, item name\n",
    "        salaryrange = salaryrange.replace(',', '') # Removes comma so can turn into int\n",
    "        salaryrange = salaryrange.replace('-', ' ').split(' ') # splits text based on dash or space\n",
    "        salaryrangelow = int(salaryrange[0])\n",
    "        salaryrangehigh = int(salaryrange[1])\n",
    "    \n",
    "        return salaryrangelow,salaryrangehigh # return as a tuple - get away from global variables!\n",
    "    \n",
    "    except (KeyError,NameError,TypeError): #If it can't find job, will return 0\n",
    "        salaryrangelow = int(0) \n",
    "        salaryrangehigh = int(0)\n",
    "    \n",
    "        return salaryrangelow,salaryrangehigh # return as a tuple - get away from global variables!\n",
    "    \n",
    "def get_url(position, location):\n",
    "    \"\"\"Generate a url from position and location\"\"\"\n",
    "    global search_word # this is for record function\n",
    "    template = 'https://www.seek.co.nz/{}-jobs/in-All-{}'\n",
    "    url = template.format(position, location)\n",
    "    search_word = position\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_record(card):\n",
    "    \"\"\"Extract job data from a single record\"\"\"\n",
    "    global searchResult2 # this is in main function to instruct if CSV exists upon DF load attempt\n",
    "    global search_word # this is from get_url function to add search word to record\n",
    "    global result_to_print # this is ending some text to main to have a user friendly print statement\n",
    "    searchResult = '' # needed only if there is no csv\n",
    "    result_to_print = '' # needed only if there is no csv\n",
    "    \n",
    "    job_id = int(card.get('data-job-id')) # this is what df looks at - as it is unique id - get function as it is high level\n",
    "    \n",
    "\n",
    "    #If there IS a CSV file, run database lookup\n",
    "    if searchResult2 != 'No existing csv file found': \n",
    "        searchResult = lookup_id(job_id)\n",
    "        result_to_print = 'There is a CSV File - Record '+str(searchResult)\n",
    "        \n",
    "    #If there is no match found in df, OR NO CSV file, mine data\n",
    "    if searchResult == 'Not found' or searchResult2 == 'No existing csv file found' : \n",
    "        \n",
    "        job_title = card.find(attrs={\"data-automation\": \"jobTitle\"}).text\n",
    "        job_url = 'https://www.seek.co.nz' + card.find(attrs={\"data-automation\": \"jobTitle\"}).get('href')\n",
    "        \n",
    "        try:\n",
    "            company = card.find(attrs={\"data-automation\": \"jobCompany\"}).text\n",
    "        except AttributeError:\n",
    "            company = ''\n",
    "\n",
    "        location = card.find(attrs={\"data-automation\": \"jobLocation\"}).text\n",
    "\n",
    "        try:\n",
    "            job_salary = card.find(attrs={\"data-automation\": \"jobSalary\"}).text\n",
    "        except AttributeError:\n",
    "            job_salary = ''\n",
    "\n",
    "        job_category = card.find(attrs={\"data-automation\": \"jobClassification\"}).text\n",
    "        job_subcategory = card.find(attrs={\"data-automation\": \"jobSubClassification\"}).text\n",
    "        job_short_description = card.find(attrs={\"data-automation\": \"jobShortDescription\"}).text\n",
    "\n",
    "        try:\n",
    "            job_listing_date = card.find(attrs={\"data-automation\": \"jobListingDate\"}).text\n",
    "        except AttributeError:\n",
    "            job_listing_date = 'Featured'\n",
    "\n",
    "        date_mined = datetime.today().strftime('%Y-%m-%d')\n",
    "        \n",
    "        salaryresult = get_salary(job_id) #Mine salary from https://www.whatsthesalary.com/\n",
    "        salary_low = salaryresult[0]\n",
    "        salary_high = salaryresult[1]\n",
    "        \n",
    "        bullet_points = ''\n",
    "        x = 0\n",
    "        for li in card.select('li'):\n",
    "            if x == 0:\n",
    "                bullet_points = bullet_points + li.text\n",
    "                x = 1\n",
    "            else:\n",
    "                bullet_points = bullet_points + ' - ' + li.text\n",
    "                \n",
    "        #Adds all mined data to list\n",
    "        record = (job_id,search_word,job_title,company,location,job_listing_date,date_mined,job_short_description,\n",
    "                  bullet_points,job_salary,salary_low,salary_high,job_category,job_subcategory,job_url)\n",
    "        \n",
    "        # compiles all the text to let user know information on what happend\n",
    "        result_to_print = result_to_print+' - Added as New Record'\n",
    "       \n",
    "       \n",
    "        return record\n",
    "    \n",
    "    \n",
    "def print_statusline(msg: str): #https://stackoverflow.com/a/43952192/6153315\n",
    "    \"\"\"Prints text inline and deletes properly\"\"\"    \n",
    "    last_msg_length = len(print_statusline.last_msg) if hasattr(print_statusline, 'last_msg') else 0\n",
    "    print(' ' * last_msg_length, end='\\r')\n",
    "    print(msg, end='\\r')\n",
    "#     sys.stdout.flush()  # Some say they needed this, I didn't.\n",
    "    print_statusline.last_msg = msg\n",
    "    \n",
    "\n",
    "    \n",
    "def main(position,location):\n",
    "    \"\"\"Run the main program routine\"\"\"\n",
    "    global searchResult2 # sends this to record function to let it know if there isn't a CSV file\n",
    "    global result_to_print # this is for user friendly print - from record function\n",
    "    global df # this is sending the df to lookup_id function - so df is only loaded once\n",
    "    searchResult2 = '' # else won't be defined if there is a CSV file causing errors\n",
    "    \n",
    "    # Loads CSV and dataframe\n",
    "    try:\n",
    "        df = pd.read_csv('results.csv')  \n",
    " \n",
    "    except FileNotFoundError:\n",
    "        searchResult2 = 'No existing csv file found'\n",
    "        print(searchResult2)\n",
    "     \n",
    "    # Getting the url to mine    \n",
    "    records = []\n",
    "    url = get_url(position, location)\n",
    "    print(url)\n",
    "    \n",
    "    # extract the job data\n",
    "    while True:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        cards = soup.find_all('article')\n",
    "        x = 1\n",
    "        for card in cards: \n",
    "            time.sleep(2)\n",
    "            record = get_record(card)\n",
    "            if record is not None:   \n",
    "                records.append(record)   \n",
    "            \n",
    "            # this is printing some user friendly, inline text\n",
    "            progressupdate = str(x)+'/'+str(len(cards)) +' - '+result_to_print\n",
    "            result_to_print = ''\n",
    "            print_statusline(progressupdate)\n",
    "\n",
    "            x = x + 1\n",
    "            \n",
    "        # will try to get next seek page, until it can't find the next button    \n",
    "        try:\n",
    "            url = 'https://www.seek.co.nz' + soup.find(attrs={\"data-automation\": \"page-next\"}).get('href')\n",
    "            print(url)\n",
    "        except AttributeError:\n",
    "            print('')\n",
    "            print('no more pages, saving data')\n",
    "            break\n",
    "            \n",
    "            \n",
    "    # save the job data and creates titles/csv file if not already created\n",
    "    if searchResult2 == 'No existing csv file found':\n",
    "        with open('results.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['ID','SearchWord','JobTitle', 'Company', 'Location', 'DateListed', 'DateMined', \n",
    "                             'ShortDesc', 'BulletPoints','AdverSalary','LowSalary','HighSalary','Cat','SubCat','URL'])\n",
    "            writer.writerows(records)\n",
    "            print('saved and done')\n",
    "    \n",
    "    else:\n",
    "        with open('results.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(records)\n",
    "            print('saved and done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run the main program\n",
    "main('Python','New-Zealand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterate through a list of searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ideally, should be in order of my granulality - as I want it label jobs as accurately as possible. \n",
    "# \"Python\" will mine tons, while \"Python Developer\" won't be as many and the Search_Keyword will be labled more accurately.\n",
    "# So start with specific keywords, then go broader. Exactly how you would search!\n",
    "\n",
    "searchlist = ['Data-Scientist','Automation-Analyst','Continuous-Improvement-Analyst',\n",
    "              'CI-Analyst','Operations-Analyst','Operation-Analyst', 'Python-Developer',\n",
    "              'Commercial-Analyst','Business-Analyst','Data-Analyst','Python']\n",
    "\n",
    "#ideas for more - Data-Developer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for search in searchlist:\n",
    "    print(search)\n",
    "    main(search,'New-Zealand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
